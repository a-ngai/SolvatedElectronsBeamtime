{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set-up for Intensity-binning analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# uncomment the following line when you want to interact with the matplotlib plots\n",
        "#%matplotlib widget\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from scipy.integrate import trapezoid\n",
        "import scipy.constants as spc\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
        "from matplotlib.colors import LogNorm, BASE_COLORS\n",
        "from matplotlib import colormaps\n",
        "cmap = colormaps.get_cmap('plasma')\n",
        "\n",
        "# import lmfit\n",
        "\n",
        "from scipy.interpolate import CubicSpline, PchipInterpolator\n",
        "from scipy.signal import convolve as conv1d\n",
        "from scipy.special import erf\n",
        "\n",
        "from fermi_libraries.run_module import Run, RunSets\n",
        "from fermi_libraries.common_functions import (rebinning, tof_mq_calibration, get_colour, \n",
        "                              add_ke_label, gaussians, residuals)\n",
        "from fermi_libraries.dictionary_search import search_symbols"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function definitions (that you might change)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def keyword_functions(keyword, aliasFunc, DictionaryObject):\n",
        "    \n",
        "\n",
        "    if False:\n",
        "        pass\n",
        "    \n",
        "    elif keyword=='bunch_parity':\n",
        "        bunches = DictionaryObject['bunches'][()]\n",
        "        parity = bunches%2==0\n",
        "\n",
        "        return parity\n",
        "    \n",
        "    elif keyword=='fel_wavelengths_mu':\n",
        "        fel_spectrum = DictionaryObject['photon_diagnostics/Spectrometer/hor_spectrum'][()]\n",
        "        padres_span = DictionaryObject['/photon_diagnostics/Spectrometer/WavelengthSpan'][()]\n",
        "        padres_wavelength = DictionaryObject['/photon_diagnostics/Spectrometer/Wavelength'][()] + 0.0575\n",
        "        padres_pixel2micron = DictionaryObject['/photon_diagnostics/Spectrometer/Pixel2micron'][()]\n",
        "        padres_lambda = padres_wavelength + np.arange(-500, 500) * padres_pixel2micron * padres_span / 1000\n",
        "        fel_lambda = padres_wavelength + np.arange(-500, 500) * padres_pixel2micron * padres_span / 1000\n",
        "\n",
        "        L = 0.5\n",
        "        above_half_max = fel_spectrum > np.max(fel_spectrum) * L\n",
        "        moment_0 = np.sum(fel_lambda**0 * fel_spectrum * above_half_max, axis=1)\n",
        "        moment_1 = np.sum(fel_lambda**1 * fel_spectrum * above_half_max, axis=1)\n",
        "\n",
        "        mu = moment_1 / moment_0\n",
        "\n",
        "        return mu\n",
        "\n",
        "    elif keyword=='fel_wavelengths_sigma':\n",
        "        fel_spectrum = DictionaryObject['photon_diagnostics/Spectrometer/hor_spectrum'][()]\n",
        "        padres_span = DictionaryObject['/photon_diagnostics/Spectrometer/WavelengthSpan'][()]\n",
        "        padres_wavelength = DictionaryObject['/photon_diagnostics/Spectrometer/Wavelength'][()] + 0.0575\n",
        "        padres_pixel2micron = DictionaryObject['/photon_diagnostics/Spectrometer/Pixel2micron'][()]\n",
        "        fel_lambda = padres_wavelength + np.arange(-500, 500) * padres_pixel2micron * padres_span / 1000\n",
        "\n",
        "        L = 0.5  # include all data points from 0 < L*max < max\n",
        "        above_half_max = fel_spectrum > np.max(fel_spectrum) * L\n",
        "        moment_0 = np.sum(fel_lambda**0 * fel_spectrum * above_half_max, axis=1)\n",
        "        moment_1 = np.sum(fel_lambda**1 * fel_spectrum * above_half_max, axis=1)\n",
        "        moment_2 = np.sum(fel_lambda**2 * fel_spectrum * above_half_max, axis=1)\n",
        "        \n",
        "        biased_sigma = np.sqrt(1/moment_0 * (moment_2 - moment_1**2/moment_0))\n",
        "        bias_correction = 1 / np.sqrt(1 - np.sqrt(-np.log(L)/np.pi) * 2*L / (erf(np.sqrt(-np.log(L)))) )\n",
        "        sigma = biased_sigma * bias_correction\n",
        "        mu = moment_1 / moment_0\n",
        "\n",
        "        return sigma\n",
        "\n",
        "    elif keyword=='seed_wavelengths_mu':\n",
        "        seed_spectrum = DictionaryObject['photon_source/SeedLaserSpectrum_FEL01/WaveMeta'][()]\n",
        "        seed_lambda = DictionaryObject['photon_source/SeedLaserSpectrum_FEL01/LambdaMeta'][()]\n",
        "        \n",
        "        L = 0.5  # include all data points from 0 < L*max < max\n",
        "        above_half_max = seed_spectrum > np.max(seed_spectrum) * L\n",
        "        moment_0 = np.sum(seed_lambda**0 * seed_spectrum * above_half_max)\n",
        "        moment_1 = np.sum(seed_lambda**1 * seed_spectrum * above_half_max)\n",
        "\n",
        "        mu = moment_1 / moment_0\n",
        "\n",
        "        return mu\n",
        "\n",
        "    elif keyword=='seed_wavelengths_sigma':\n",
        "        seed_spectrum = DictionaryObject['photon_source/SeedLaserSpectrum_FEL01/WaveMeta'][()]\n",
        "        seed_lambda = DictionaryObject['photon_source/SeedLaserSpectrum_FEL01/LambdaMeta'][()]\n",
        "        \n",
        "        L = 0.5\n",
        "        above_half_max = seed_spectrum > np.max(seed_spectrum) * L\n",
        "        moment_0 = np.sum(seed_lambda**0 * seed_spectrum * above_half_max)\n",
        "        moment_1 = np.sum(seed_lambda**1 * seed_spectrum * above_half_max)\n",
        "        moment_2 = np.sum(seed_lambda**2 * seed_spectrum * above_half_max)\n",
        "        \n",
        "        biased_sigma = np.sqrt(1/moment_0 * (moment_2 - moment_1**2/moment_0))\n",
        "        bias_correction = 1 / np.sqrt(1 - np.sqrt(-np.log(L)/np.pi) * 2*L / (erf(np.sqrt(-np.log(L)))) )\n",
        "        sigma = biased_sigma * bias_correction\n",
        "        mu = moment_1 / moment_0\n",
        "\n",
        "        return sigma\n",
        "\n",
        "    elif keyword=='fel_wavelengths':\n",
        "        padres_span = DictionaryObject['/photon_diagnostics/Spectrometer/WavelengthSpan'][()]\n",
        "        padres_wavelength = DictionaryObject['/photon_diagnostics/Spectrometer/Wavelength'][()] + 0.0575\n",
        "        padres_pixel2micron = DictionaryObject['/photon_diagnostics/Spectrometer/Pixel2micron'][()]\n",
        "        padres_lambda = padres_wavelength + np.arange(-500, 500) * padres_pixel2micron * padres_span / 1000\n",
        "\n",
        "        return padres_lambda\n",
        "\n",
        "    elif keyword=='total_retardation':\n",
        "        voltage_1 = DictionaryObject['endstation/MagneticBottle/voltage_ch1'][()]\n",
        "        voltage_2 = DictionaryObject['endstation/MagneticBottle/voltage_ch2'][()]\n",
        "        voltage_3 = DictionaryObject['endstation/MagneticBottle/voltage_ch3'][()]\n",
        "        voltage_1_on = DictionaryObject['endstation/MagneticBottle/ch1_is_enabled'][()]\n",
        "        voltage_2_on = DictionaryObject['endstation/MagneticBottle/ch2_is_enabled'][()]\n",
        "        voltage_3_on = DictionaryObject['endstation/MagneticBottle/ch3_is_enabled'][()]\n",
        "        retardation = voltage_1*voltage_1_on + voltage_2*voltage_2_on - voltage_3*voltage_3_on\n",
        "\n",
        "        return retardation\n",
        "\n",
        "    \n",
        "    elif keyword=='bunch_parity':\n",
        "        bunches = DictionaryObject['bunches'][()]\n",
        "        parity = bunches%2==0\n",
        "\n",
        "        return parity\n",
        "    \n",
        "\n",
        "    else:\n",
        "        return DictionaryObject[aliasFunc(keyword)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def spectra_subtraction(foreground,background):\n",
        "    return -(foreground-background)\n",
        "\n",
        "def remove_slu_cycling(data):\n",
        "    '''\n",
        "    Remove the part corresponding to the keyword \"slu_sep=True\" in the Run() methods.\n",
        "    For reference:\n",
        "        data = (ff_fs, bf_fs, ff_bs, bf_bs)\n",
        "    where:\n",
        "        ff = FEL ON  (foreground fel)\n",
        "        bf = FEL OFF (background fel)\n",
        "\n",
        "        _fs = SLU ON  (foreground slu)\n",
        "        _bs = SLU OFF (background slu)\n",
        "    '''\n",
        "\n",
        "    return data[:2]\n",
        "\n",
        "def swap_rules_runs(data, single_rule=False, single_run=False, single_shot = False, file_level=False):\n",
        "    '''\n",
        "    Takes either the axes conditions:\n",
        "        axes = (conditions, runs, rules, data)\n",
        "    and returns the axes:\n",
        "        axes = (conditions, rules, runs, data)\n",
        "    OR the axes conditions:\n",
        "        axes = (conditions, rules, data)\n",
        "    and returns the axes:\n",
        "        axes = (conditions, rules, data)\n",
        "\n",
        "   IMPORTANT: \n",
        "        The names \"single_rule\" and \"single_run\" make sense ONLY for the averaging functions\n",
        "            e.g. Run.average_rundata_weights()\n",
        "            i.e. the data has axes average_data = (conditions, runs, rules, shot_data).\n",
        "        When you use this on the file-yielding generators, the axes are different\n",
        "            e.g. Run.yield_filedata()\n",
        "            i.e. the data has axes file_data = (conditions, rules, shot_data).\n",
        "        So when using this on the file-yielding generators, treat \n",
        "            \"single_run\" -> \"single_rule\"\n",
        "            \"single_rule\"-> \"single_shot\"\n",
        "\n",
        "    single_rule : bool (optional)\n",
        "        if True, collapses the \"rules\" dimension of the data and only gives the first element\n",
        "    '''\n",
        "    output = []\n",
        "    for condition in data:\n",
        "        ndim = np.ndim(data[0])\n",
        "\n",
        "        if file_level:\n",
        "            if ndim < 2:\n",
        "                raise Exception(f'ndim of condition ({ndim}) should be >=2 if file_level=True')\n",
        "\n",
        "            transposed = np.array(condition)\n",
        "            if single_shot:\n",
        "                transposed = transposed[:,0]\n",
        "            if single_rule:\n",
        "                transposed = transposed[0]\n",
        "            output.append(np.array(transposed))\n",
        "\n",
        "        elif not file_level and ndim < 2:\n",
        "            raise Exception(f'ndim of condition ({ndim}) should be >=2 if file_level=False')\n",
        "        else:\n",
        "\n",
        "            transposed = np.array(np.swapaxes(condition, 1,0))\n",
        "            if single_shot and ndim < 4:\n",
        "                print('single_shot keyword not valid here')\n",
        "            elif single_shot:\n",
        "                transposed = transposed[:,:,0]\n",
        "            if single_run:\n",
        "                transposed = transposed[:,0]\n",
        "            if single_rule:\n",
        "                transposed = transposed[0]\n",
        "            output.append(np.array(transposed))\n",
        "\n",
        "    return output\n",
        "\n",
        "def simplify_data(data, single_rule=False, single_run=False, file_level=False):\n",
        "    return swap_rules_runs(remove_slu_cycling(data), \n",
        "                           single_rule=single_rule, single_run=single_run, file_level=file_level)\n",
        "\n",
        "def nm_to_ev(nm):\n",
        "    '''Convert from nm to eV.'''\n",
        "    return spc.h*spc.c/spc.nano/spc.e/nm\n",
        "\n",
        "def ev_to_nm(eV):\n",
        "    '''Convert from eV to nm.'''\n",
        "    return spc.h*spc.c/spc.nano/spc.e/eV"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alias definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Figure bookkeeping to save memory\n",
        "figs = {}\n",
        "def newfig(id, *args, **kwargs):\n",
        "    id = 0\n",
        "    if id in figs:\n",
        "        plt.close(figs[id].number)\n",
        "    fig, ax = plt.subplots(*args, **kwargs)\n",
        "    figs.update({id: fig})\n",
        "    return fig, ax\n",
        "\n",
        "# Alternative names for the HDF5 groupnames\n",
        "alias_dict = {\n",
        "    'i0m' : 'photon_diagnostics/FEL01/I0_monitor/iom_sh_a',\n",
        "    'i0m_current' : 'photon_diagnostics/FEL01/I0_monitor/iom_sh_a_pc',\n",
        "    'vmi' : 'vmi/andor',\n",
        "    'ion_tof' : 'digitizer/channel1',\n",
        "    'delay' : 'user_laser/delay_line/position',\n",
        "    'slu' : 'user_laser/energy_meter/Energy2',\n",
        "    'fel_spectrometer' : 'photon_diagnostics/Spectrometer/hor_spectrum',\n",
        "    'fel_wavelength' : 'photon_source/FEL01/wavelength',\n",
        "    'seed_spectrometer' : 'photon_source/SeedLaserSpectrum_FEL01/WaveMeta',\n",
        "    'seed_wavelength' : 'photon_source/SeedLaser/Wavelength',\n",
        "    'seed_wavelengths' : 'photon_source/SeedLaserSpectrum_FEL01/LambdaMeta',\n",
        "    'harmonic_number' : 'photon_source/FEL01/harmonic_number',\n",
        "    'bunch_number' : 'bunches',\n",
        "    'pressure' : 'photon_diagnostics/FEL01/Gas_Attenuator/Pressure',\n",
        "    'poletto' : 'cosp/HorSpectrum',\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------------------------------------------\n",
        "# ! Data selection ! -----------------------------------------------------------------------------\n",
        "\n",
        "This block contains the variables you might change every different Run. \n",
        "Changing \"ion_tof_range\" or \"eon_tof_range\" __does not__ make the program run faster; we are limited\n",
        "by the compression in FERMI's HDF5 files. If working memory is a problem, then decrease these\n",
        "ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# BEAMTIME_DIR =  '/net/online4ldm/store/20209112b/results/TestData/'\n",
        "BEAMTIME_DIR =  'TestBeamtime/'\n",
        "DATA_DIR = BEAMTIME_DIR+'Beamtime/'  # change from fictitious to the real raw data directory!\n",
        "SAVE_DIR = BEAMTIME_DIR+'results/evaluation/'#'/net/online4ldm/store/20209134/results/results' # ditto\n",
        "\n",
        "SAVE_FILES = False\n",
        "\n",
        "BACKGROUND = True  # Only set to False if you want to sum up everything\n",
        "NAMEADD = 'test' # your name here\n",
        "run_numbers = np.arange(1,3)\n",
        "\n",
        "\n",
        "# variables for data extraction ans rebinning\n",
        "ION_TOF_REBIN = 10\n",
        "ion_tof_range = (4000, 30000, 1) # select ion tof range for plotting\n",
        "new_ion_mq = np.linspace(0.1,200,num=1200)\n",
        "\n",
        "EON_TOF_REBIN = 1\n",
        "eon_tof_range = (4000, 10000, 1)\n",
        "new_eKE = np.linspace(0.5, 50, num=400)\n",
        "\n",
        "ion_tof_slices = [ion_tof_range]\n",
        "eon_tof_slices = [eon_tof_range]\n",
        "\n",
        "ion_tof = np.arange(*ion_tof_slices[0])\n",
        "rebin_ion_tof = ion_tof[::ION_TOF_REBIN]\n",
        "eon_tof = np.arange(*eon_tof_slices[0])\n",
        "rebin_eon_tof = eon_tof[::EON_TOF_REBIN]\n",
        "\n",
        "MAKE_CACHE = True\n",
        "LOAD_FROM_CACHE = False\n",
        "\n",
        "calibration_run_number = 1\n",
        "\n",
        "print(run_numbers)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create RunCollection (main data structure), and print location of our save directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This block loads all the relevent HDF5 filepaths into their respective Run\n",
        "# (Each Run has more than a single HDF5 file! Usually less than 100.)\n",
        "\n",
        "RunCollection = {}  # We will put all the 'Runs' in thes dictionary\n",
        "for run_id in (list(run_numbers) + [calibration_run_number,]):\n",
        "    folderpath = os.path.join(DATA_DIR, f'Run_{run_id:03d}/rawdata')\n",
        "    filepaths = [folderpath+'/'+filename for filename in os.listdir(folderpath)[::]]\n",
        "    RunCollection[run_id] = Run(filepaths,\n",
        "                                alias_dict=alias_dict, search_symbols=search_symbols,\n",
        "                                keyword_functions=keyword_functions,\n",
        "                                )  # create a Run object with its respective filepaths\n",
        "\n",
        "# This creates a set out of the run_numbers selected above\n",
        "Set1 = RunSets([])\n",
        "for run in run_numbers:\n",
        "    Set1.add([RunCollection[run]])\n",
        "print(f'Data set contains {len(Set1.run_instances)} run(s).')\n",
        "\n",
        "if len(run_numbers) > 1:\n",
        "    run_string = f\"run_{min(run_numbers):04d}-{max(run_numbers):04d}\"\n",
        "elif run_numbers:\n",
        "    run_string = f\"run_{run_numbers[0]:04d}\"\n",
        "else:\n",
        "    raise IndexError('no run numbers???')\n",
        "\n",
        "prefix = os.path.join(SAVE_DIR, run_string)\n",
        "outdir = (prefix + '_' + NAMEADD).rstrip('_')\n",
        "print(f'Save directory: ...{outdir[30:]}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create directory if non-existent (and if we are actually saving files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if SAVE_FILES:\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------------------------------------------\n",
        "# Show the average background-subtracted electron TOF for each Run ---------------------\n",
        "\n",
        "The output for Runset.averageRunData and Runset.average_run_data_weights has the\n",
        "axes shape (rule, condition, run, data):\n",
        "\"rule\" are the filtering rules\n",
        "\"condition\" is in the order (FEL:ON SLU:ON, FEL:OFF SLU:ON, FEL:ON SLU:OFF, FEL:OFF SLU:OFF)\n",
        "\"run\" are the individual Runs\n",
        "\"data\" is the average rundata/weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "runset_ion_tof_data = Set1.average_run_data('ion_tof', back_sep=BACKGROUND,\n",
        "                                    slice_range=ion_tof_slices,\n",
        "                                    make_cache=MAKE_CACHE, use_cache=LOAD_FROM_CACHE)\n",
        "fore_ion_rundata, back_ion_rundata = simplify_data(runset_ion_tof_data, single_rule=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "overall_integral_eKE = []\n",
        "# fel_energys = nm_to_ev(fel_wavelengths)\n",
        "\n",
        "subt_ion_tof_rundata = -(fore_ion_rundata - back_ion_rundata)\n",
        "rebin_ion_tof_rundata = rebinning(rebin_ion_tof, ion_tof, subt_ion_tof_rundata, axis=1)\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
        "for (runnumber, ion_spectrum_tof) in zip(\n",
        "    run_numbers, rebin_ion_tof_rundata):\n",
        "    ax.plot(rebin_ion_tof, ion_spectrum_tof, label=f\"Run_{runnumber:03d}\")\n",
        "\n",
        "ax.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0, ncol = 2)\n",
        "ax.set_xlabel('ion TOF')\n",
        "ax.set_ylabel('ion TOF signal; rebinned (arb.u.)')\n",
        "ax.set_title(f'Runs {run_numbers[0]}-{run_numbers[-1]} Average of the complete Run')\n",
        "\n",
        "# if SAVE_FILES:\n",
        "#     fig.savefig(outdir+'/Average_of_complete_run.png')\n",
        "#     fig1.savefig(outdir+'/Average_of_complete_run_eV.png')\n",
        "ax.set_ylim(-1,1)\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def closest(locs, array):\n",
        "    indices = [np.where(np.min((loc-array)**2) == (loc-array)**2)[0][0] for loc in locs]\n",
        "    return np.array(indices)\n",
        "\n",
        "ion_tof_mq_peaks = np.array([\n",
        "    # [5000, 999],\n",
        "    [6000, 0],\n",
        "    [10500, 14],\n",
        "    [12000, 28],\n",
        "    [13100, 36],\n",
        "\n",
        "])\n",
        "tof_points, mq_points = ion_tof_mq_peaks.T\n",
        "\n",
        "ion_cal_rundata = RunCollection[calibration_run_number].average_run_data('ion_tof', back_sep=BACKGROUND,\n",
        "                                    slice_range=ion_tof_slices,\n",
        "                                    make_cache=MAKE_CACHE, use_cache=LOAD_FROM_CACHE)\n",
        "fore_ion_rundata, back_ion_rundata = simplify_data(ion_cal_rundata)\n",
        "cal_sub_spectrum = back_ion_rundata[:,0] - fore_ion_rundata[:,0]\n",
        "\n",
        "\n",
        "ion_calibration_dict = tof_mq_calibration(peaks=ion_tof_mq_peaks)\n",
        "(tof_mq_coor_func, tof_mq_jaco_func,\n",
        " mq_tof_coor_func, mq_tof_jaco_func,\n",
        " ion_constants_dict) = list(ion_calibration_dict.values())\n",
        "print(f'calibration constants:  {ion_constants_dict}')\n",
        "ion_constants = ion_constants_dict['timezero'], ion_constants_dict['C']\n",
        "\n",
        "model_tof = np.linspace(np.min(tof_points), np.max(tof_points), num=1000)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,4))\n",
        "ax1.plot(tof_points, cal_sub_spectrum[closest(tof_points, ion_tof)], marker='v', linestyle='')\n",
        "ax1.plot(ion_tof, cal_sub_spectrum)\n",
        "# ax1.set_xlim(5000,7000)\n",
        "ax1.set_title('calibration points')\n",
        "ax1.set_xlabel('tof (ns)')\n",
        "ax1.set_ylabel('tof (ns)')\n",
        "ax2.plot(tof_points, mq_points, marker='o', linestyle='')\n",
        "ax2.plot(model_tof, tof_mq_coor_func(model_tof), color='black')\n",
        "ax2.set_title('calibration fit')\n",
        "ax2.set_xlabel('tof (ns)')\n",
        "ax2.set_ylabel('mq (m/q)')\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Using ion constants: (t0, C) = {ion_constants}\")\n",
        "# print(f\"Using eon constants: (t0, C, E0) = {eon_constants}\")\n",
        "\n",
        "ion_calibration_dict = tof_mq_calibration(constants=ion_constants)\n",
        "(tof_mq_coor_func, tof_mq_jaco_func,\n",
        " mq_tof_coor_func, mq_tof_jaco_func,\n",
        " ion_constants_dict) = list(ion_calibration_dict.values())\n",
        "\n",
        "# eon_calibration_dict = tof_ke_calibration(constants=eon_constants)\n",
        "# (tof_ke_coor_func, tof_ke_jaco_func,\n",
        "#  ke_tof_coor_func, ke_tof_jaco_func,\n",
        "#  eon_constants_dict) = list(eon_calibration_dict.values())\n",
        "\n",
        "def transpose_axis(y, axis=None):\n",
        "    if (axis is None) or (np.ndim(y)==1):\n",
        "        transpose = y\n",
        "    else:\n",
        "        transpose_order = np.arange(np.ndim(y))\n",
        "        if axis!=0:\n",
        "            transpose_order[0]=axis\n",
        "            transpose_order[axis]=0\n",
        "        transpose = np.transpose(y, axes=transpose_order)\n",
        "    return transpose\n",
        "\n",
        "# def tof_to_eke(tof, spectrum, axis=None):\n",
        "#     \"\"\" Helper function that converts TOF into eKE spectra with the Jacobian correction \"\"\"\n",
        "#     spectrum = transpose_axis(spectrum, axis=axis)\n",
        "#     eke = tof_ke_coor_func(tof)\n",
        "#     mask = eke>eon_constants_dict['KE0']\n",
        "#     jacobian = tof_ke_jaco_func(tof)\n",
        "#     if np.ndim(spectrum)>1:\n",
        "#         jacobian = jacobian[:,np.newaxis]\n",
        "#     eke_coor = eke[mask]\n",
        "#     eke_spec = (spectrum * jacobian)[mask]\n",
        "#     eke_spec = transpose_axis(eke_spec, axis=axis)  # revert transposition\n",
        "#     return eke_coor, eke_spec\n",
        "\n",
        "# def eke_to_tof(eke, spectrum, axis=None):\n",
        "#     \"\"\" Helper function that converts eKE into TOF spectra with the Jacobian correction \"\"\"\n",
        "#     spectrum = transpose_axis(spectrum, axis=axis)\n",
        "#     tof = ke_tof_coor_func(eke)\n",
        "#     mask = eke>0\n",
        "#     jacobian = ke_tof_jaco_func(eke)\n",
        "#     if np.ndim(spectrum)>1:\n",
        "#         jacobian = jacobian[:,np.newaxis]\n",
        "#     tof_coor = tof[mask]\n",
        "#     tof_spec = (spectrum * jacobian)[mask]\n",
        "#     tof_spec = transpose_axis(tof_spec, axis=axis)  # revert transposition\n",
        "#     return tof_coor, tof_spec\n",
        "\n",
        "def tof_to_mq(tof, spectrum, axis=None):\n",
        "    \"\"\" Helper function that converts TOF into eKE spectra with the Jacobian correction \"\"\"\n",
        "    spectrum = transpose_axis(spectrum, axis=axis)\n",
        "    mq = tof_mq_coor_func(tof)\n",
        "    mask = tof>ion_constants_dict['timezero']\n",
        "    jacobian = tof_mq_jaco_func(tof)\n",
        "    if np.ndim(spectrum)>1:\n",
        "        jacobian = jacobian[:,np.newaxis]\n",
        "    mq_coor = mq[mask]\n",
        "    mq_spec = (spectrum * jacobian)[mask]\n",
        "    mq_spec = transpose_axis(mq_spec, axis=axis)  # revert transposition\n",
        "    return mq_coor, mq_spec\n",
        "\n",
        "def mq_to_tof(eke, spectrum, axis=None):\n",
        "    \"\"\" Helper function that converts eKE into TOF spectra with the Jacobian correction \"\"\"\n",
        "    spectrum = transpose_axis(spectrum, axis=axis)\n",
        "    tof = mq_tof_coor_func(eke)\n",
        "    mask = tof>ion_constants_dict['timezero']\n",
        "    jacobian = mq_tof_jaco_func(mq)\n",
        "    if np.ndim(spectrum)>1:\n",
        "        jacobian = jacobian[:,np.newaxis]\n",
        "    tof_coor = tof[mask]\n",
        "    tof_spec = (spectrum * jacobian)[mask]\n",
        "    tof_spec = transpose_axis(tof_spec, axis=axis)  # revert transposition\n",
        "    return tof_coor, tof_spec"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,4))\n",
        "for (runnumber, ion_spectrum_tof) in zip(\n",
        "    run_numbers, rebin_ion_tof_rundata):\n",
        "    ax1.plot(rebin_ion_tof, ion_spectrum_tof, label=f\"Run_{runnumber:03d}\")\n",
        "\n",
        "ax1.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0, ncol = 2)\n",
        "ax1.set_xlabel('ion TOF')\n",
        "ax1.set_ylabel('ion TOF signal; rebinned (arb.u.)')\n",
        "ax1.set_title(f'Runs {run_numbers[0]}-{run_numbers[-1]} Average of the complete Run')\n",
        "ax1.set_yscale('log')\n",
        "ylim = ax1.set_ylim()\n",
        "ax1.set_ylim(ylim[1]*1e-4, ylim[1])\n",
        "\n",
        "for (runnumber, ion_spectrum_tof) in zip(\n",
        "    run_numbers, rebin_ion_tof_rundata):\n",
        "    mq_coor, mq_spectrum = tof_to_mq(rebin_ion_tof, -ion_spectrum_tof)\n",
        "    rebin_mq = np.linspace(0.1, 70, num=1000)\n",
        "    rebin_mq_spectrum = rebinning(rebin_mq, mq_coor, mq_spectrum)\n",
        "    ax2.plot(rebin_mq, rebin_mq_spectrum, label=f\"Run_{runnumber:03d}\")\n",
        "\n",
        "ax2.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0, ncol = 2)\n",
        "ax2.set_xlabel('mq')\n",
        "ax2.set_ylabel('ion TOF signal; rebinned (arb.u.)')\n",
        "ax2.set_yscale('log')\n",
        "ylim = ax2.set_ylim()\n",
        "ax2.set_ylim(ylim[1]*1e-4, ylim[1])\n",
        "ax2.set_title(f'Runs {run_numbers[0]}-{run_numbers[-1]} Average of the complete Run')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ------------------------------------------------------------------------------------------------\n",
        "# I0M filtering -----------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "raw_i0m_rundata = Set1.give_rundata('i0m', make_cache=MAKE_CACHE, use_cache=LOAD_FROM_CACHE)\n",
        "try:\n",
        "    i0m_runset_data, _ = simplify_data(raw_i0m_rundata, single_rule=True)  # can use simplify_data, because no rules\n",
        "except:\n",
        "    fore_rundata_i0m, back_rundata_iom, *_ = raw_i0m_rundata\n",
        "    i0m_runset_data = fore_rundata_i0m\n",
        "    for i in range(len(i0m_runset_data)):  # collapse the rule dimension\n",
        "        i0m_runset_data[i] = i0m_runset_data[i][0]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check the range of possible I0M intensities here; change I0M binning in the next block if needed!\n",
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,3))\n",
        "for runnumber, i0m_run_data in zip(run_numbers, i0m_runset_data):\n",
        "    i0m_data = i0m_run_data[:,0]  # just collapsing an extraneous dimension\n",
        "    ax1.plot(i0m_data, marker='o', markersize=1, linestyle='', label=f'Run_{runnumber:03d}')\n",
        "    ax2.hist(i0m_data, bins=np.linspace(min(i0m_data),max(i0m_data),num=100),\n",
        "             label=f'Run_{runnumber:03d}')\n",
        "ax1.set_ylabel('I0M intensity (uJ)')\n",
        "ax1.set_xlabel('shot number')\n",
        "ax1.set_title(f'Runs {run_numbers[0]}-{run_numbers[-1]} I0M over time')\n",
        "#ax1.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0, ncol = 2)\n",
        "\n",
        "ax2.set_ylabel('binned counts')\n",
        "ax2.set_xlabel('I0M (uJ)')\n",
        "ax2.set_title(f'Runs {run_numbers[0]}-{run_numbers[-1]} Histogram of I0M')\n",
        "#ax2.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0, ncol = 2)\n",
        "plt.tight_layout()\n",
        "\n",
        "if SAVE_FILES:\n",
        "    plt.savefig(outdir+'/I0M_time_and_bins.png')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %%time # uncomment to show time\n",
        "runset_vmi = Set1.average_run_data('vmi',back_sep=BACKGROUND,\n",
        "                                    make_cache=MAKE_CACHE, use_cache=LOAD_FROM_CACHE)\n",
        "fore_vmi, back_vmi = simplify_data(runset_vmi, single_rule=True, single_run=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sub_vmi = fore_vmi - back_vmi\n",
        "print(np.shape(sub_vmi))\n",
        "plt.imshow(np.average(sub_vmi, axis=0))\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}